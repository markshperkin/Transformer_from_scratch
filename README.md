# Transformer from Scratch Using PyTorch

## Overview

This project implements a Transformer model from scratch using PyTorch, inspired by the original "Attention is All You Need" paper. The Transformer is a deep learning model primarily used for tasks involving sequence data, such as natural language processing (NLP). It utilizes self-attention mechanisms to capture dependencies between words in a sentence, making it highly effective for tasks like machine translation, text summarization, and language modeling. The Transformer model is unique in its ability to process input data in parallel, resulting in more efficient training compared to recurrent neural networks (RNNs).

## Article Reference

This project was built with the guidance of the article [Build Your Own Transformer from Scratch Using PyTorch](https://towardsdatascience.com/build-your-own-transformer-from-scratch-using-pytorch-84c850470dcb). The article provided a clear and structured approach to implementing the Transformer model, explaining key concepts like self-attention, multi-head attention, and positional encodings.

## Class Project

This project was developed as part of the Neural Network class under the instruction of [Professor Vignesh Narayanan](https://sc.edu/study/colleges_schools/engineering_and_computing/faculty-staff/narayanan_vignesh.php) at the University of South Carolina.
